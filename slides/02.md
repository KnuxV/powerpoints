## Cours 2
September 6th 2024

---
### Information about Git
- A tool for version control
- Helps manage changes in your projects
- Essential for collaboration in software development

---
## Who Uses Git?

- Software developers of all levels
- Large tech companies (Google, Microsoft, Amazon)
- Open-source projects (Linux, Python, React)
- Startups and small businesses
- Students and researchers
- Anyone working with code or text-based projects

---
## Why Master Git?

- Industry standard in software development
- Enhances collaboration skills
- Improves project management abilities
- Valuable skill for internships and jobs

---
## Learning

- Many books and tutorials online
- (53 min) [Video by Harvard University](https://www.youtube.com/watch?v=NcoBAfJ6l2Q)
  - Explains the basics of Git very clearly
- Other free and brilliant classes from Harvard.
    - [Intro to Web Programming with Python and Javascript](https://cs50.harvard.edu/web/2020/)
    - [Intro to AI with Python](https://cs50.harvard.edu/ai/2024/)

---
### [GitHub Desktop](https://github.com/apps/desktop): Git Made Easy

- A user-friendly application for using Git
- No need to use command line
- Visualizes changes in your projects
- Simplifies collaboration through GitHub

---
### Git Exercise: 

- Create your own repository
- Upload your code from last week on GitHub 
- Upload your code from today on GitHub
- Send me the URL by e-mail/Discord

---
---
## Static vs Dynamic Content in Web Scraping

- Two main types of web content: Static and Dynamic
- Crucial to understand for effective web scraping
- Requires different approaches and tools

---
## Static Content

- HTML delivered as-is from the server
- Content doesn't change without a page reload
- What you see in 'View Source' is what you get
- Examples: Basic HTML websites, some blogs, documentation sites

---
## Dynamic Content

- Content generated or modified by JavaScript after page load
- May change without full page reload (e.g., AJAX)
- 'View Source' doesn't show the final rendered content
- Examples: Social media feeds, interactive dashboards, single-page applications (SPAs)

---
## Tools for Scraping Static Content

- Beautiful Soup: Python library for parsing HTML and XML
- Requests: HTTP library for making web requests
- lxml: Fast and feature-rich library for processing XML and HTML

```python
import requests
from bs4 import BeautifulSoup

response = requests.get('https://example.com')
soup = BeautifulSoup(response.text, 'html.parser')
```

---
## Tools for Scraping Dynamic Content

- Selenium: Automates browsers, interacts with dynamic elements
- Puppeteer: Headless Chrome Node.js API
- Scrapy with Splash: Combines Scrapy's power with JavaScript rendering

```python
from selenium import webdriver

driver = webdriver.Chrome()
driver.get('https://example.com')
element = driver.find_element_by_id('dynamic-content')
```

---
## Identifying Static vs Dynamic Content

1. View Source: If content is there, likely static
2. Chrome DevTools:
   - Network tab: Look for XHR/Fetch requests
   - Elements tab: Observe DOM changes
3. Disable JavaScript: If content disappears, it's dynamic

---
## Examples: Static Content Sites

- Wikipedia articles
- Government websites (e.g., census.gov)
- Classic news websites (e.g., older versions of BBC)
- Project documentation (e.g., Python docs)

---
## Examples: Dynamic Content Sites

- Twitter, Facebook, LinkedIn feeds
- Google Maps
- Gmail, Outlook web interfaces
- Modern e-commerce sites (e.g., Amazon product recommendations)
- Single-page applications (SPAs) like React or Angular apps

---
## Challenges with Dynamic Content

- Content loaded asynchronously
- Infinite scrolling
- User interactions required to load content
- CAPTCHAs and anti-bot measures
- Changing element IDs or classes

---
## Best Practices for Scraping Dynamic Sites

1. Use appropriate tools (Selenium, Puppeteer)
2. Implement waits for elements to load
3. Simulate user behavior (scrolling, clicking)
4. Handle AJAX requests directly if possible
5. Consider using APIs if available

---
## Performance Considerations

- Static scraping is generally faster
- Dynamic scraping requires browser rendering
- Headless browsers can improve performance
- Consider parallelization for large-scale scraping
- Respect rate limits and robots.txt

---
## Web Scraping with Selenium

- Automated data extraction from websites
- Uses Selenium, a powerful web automation tool
- Interacts with websites like a real user

---
## What is Selenium?

- Originally a tool for web application testing
- Can control a web browser through programming
- Supports multiple browsers (Chrome, Firefox, Safari, etc.)
- Allows interaction with dynamic, JavaScript-heavy websites

---
## Why Use Selenium for Web Scraping?

- Handles dynamic content loaded by JavaScript
- Can interact with forms, buttons, and other elements
- Simulates user behavior (clicking, typing, scrolling)
- Useful for sites that require login or navigation

---
## Basic Selenium Workflow

1. Import the Selenium library
2. Create a WebDriver instance (e.g., for Chrome)
3. Use the driver to navigate to a webpage
4. Find elements using various methods (ID, class, XPath)
5. Interact with or extract data from these elements
6. Close the browser when done

---
## Example Use Cases

- Automated testing of web applications
- Gathering product information from e-commerce sites
- Collecting social media data
- Monitoring websites for changes
- Automating repetitive web-based tasks

---
## Ethical Considerations

- Always check a website's robots.txt file
- Respect the site's terms of service
- Be mindful of your scraping frequency (rate limiting)
- Don't overload servers with too many requests
- Consider using APIs if available instead of scraping